# GitLab CI/CD –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Performance Testing
# –§–∞–π–ª: .gitlab-ci.yml

stages:
  - build
  - test-performance
  - report

variables:
  # Docker –æ–±—Ä–∞–∑—ã
  KOTLIN_JVM_IMAGE: "openjdk:17-jdk"
  ANDROID_IMAGE: "cimg/android:2024.01.1"

  # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  GRADLE_OPTS: "-Dorg.gradle.daemon=false -Dorg.gradle.workers.max=2"
  GRADLE_USER_HOME: "$CI_PROJECT_DIR/.gradle"
  ANDROID_COMPILE_SDK: "34"
  ANDROID_BUILD_TOOLS: "34.0.0"
  ANDROID_SDK_TOOLS: "9477386"

  # –ü–æ—Ä–æ–≥–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  PERFORMANCE_THRESHOLD: "120%"
  CRITICAL_THRESHOLD: "200%"

# –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å–±–æ—Ä–∫–∏
.cache_template: &cache_template
  cache:
    key:
      files:
        - gradle/wrapper/gradle-wrapper.properties
    paths:
      - .gradle/wrapper
      - .gradle/caches

# –ë–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è Kotlin –ø—Ä–æ–µ–∫—Ç–æ–≤
.kotlin_base:
  image: $KOTLIN_JVM_IMAGE
  <<: *cache_template
  before_script:
    - chmod +x ./gradlew
    - export GRADLE_USER_HOME="$CI_PROJECT_DIR/.gradle"

# –°–±–æ—Ä–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞
build:
  extends: .kotlin_base
  stage: build
  script:
    - ./gradlew clean build -x test
  artifacts:
    paths:
      - build/
    expire_in: 1 hour
    reports:
      junit: build/test-results/test/TEST-*.xml

# JVM –±–µ–Ω—á–º–∞—Ä–∫–∏
jvm-benchmarks:
  extends: .kotlin_base
  stage: test-performance
  dependencies:
    - build
  script:
    - echo "üöÄ Running JVM benchmarks..."
    - ./gradlew jvmBenchmark --info

    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    - echo "üìä Processing JVM benchmark results..."
    - mkdir -p reports/jvm

    # –ö–æ–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    - |
      if [ -d "build/reports/benchmarks/main" ]; then
        cp -r build/reports/benchmarks/main/* reports/jvm/
        echo "‚úÖ JVM benchmark results saved"
      else
        echo "‚ùå No JVM benchmark results found"
        exit 1
      fi

  artifacts:
    paths:
      - reports/jvm/
    expire_in: 7 days
    reports:
      junit: build/test-results/jvmBenchmark/TEST-*.xml

  # –†–∞–∑—Ä–µ—à–∞–µ–º failure –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
  allow_failure: false

# JS –±–µ–Ω—á–º–∞—Ä–∫–∏
js-benchmarks:
  extends: .kotlin_base
  stage: test-performance
  dependencies:
    - build
  script:
    - echo "üåê Running JavaScript benchmarks..."
    - ./gradlew jsBenchmark --info

    - echo "üìä Processing JS benchmark results..."
    - mkdir -p reports/js

    - |
      if [ -d "build/reports/benchmarks/js" ]; then
        cp -r build/reports/benchmarks/js/* reports/js/
        echo "‚úÖ JS benchmark results saved"
      else
        echo "‚ùå No JS benchmark results found"
        exit 1
      fi

  artifacts:
    paths:
      - reports/js/
    expire_in: 7 days

  allow_failure: true  # JS –±–µ–Ω—á–º–∞—Ä–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏

# Native –±–µ–Ω—á–º–∞—Ä–∫–∏ (—Ç–æ–ª—å–∫–æ –Ω–∞ Linux)
native-benchmarks:
  extends: .kotlin_base
  stage: test-performance
  dependencies:
    - build
  tags:
    - linux  # –¢–æ–ª—å–∫–æ –Ω–∞ Linux runners
  script:
    - echo "‚ö° Running Kotlin/Native benchmarks..."
    - ./gradlew nativeBenchmark --info

    - echo "üìä Processing Native benchmark results..."
    - mkdir -p reports/native

    - |
      if [ -d "build/reports/benchmarks/native" ]; then
        cp -r build/reports/benchmarks/native/* reports/native/
        echo "‚úÖ Native benchmark results saved"
      else
        echo "‚ùå No Native benchmark results found"
        exit 1
      fi

  artifacts:
    paths:
      - reports/native/
    expire_in: 7 days

  allow_failure: true

# Android –±–µ–Ω—á–º–∞—Ä–∫–∏ (—Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
android-benchmarks:
  image: $ANDROID_IMAGE
  stage: test-performance
  dependencies:
    - build
  variables:
    ANDROID_HOME: "/opt/android/sdk"
  before_script:
    - chmod +x ./gradlew
    - export PATH=$PATH:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools

  script:
    - echo "üì± Running Android benchmarks..."

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–º—É–ª—è—Ç–æ—Ä–∞
    - echo "Checking for available Android devices..."
    - adb devices || echo "No ADB devices found - using instrumented tests"

    # –°–±–æ—Ä–∫–∞ Android —Ç–µ—Å—Ç–æ–≤
    - ./gradlew assembleDebug assembleBenchmark
    - ./gradlew assembleDebugAndroidTest assembleBenchmarkAndroidTest

    # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ (–µ—Å–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
    - |
      if adb devices | grep -q "device$"; then
        echo "üì± Device found - running connected tests"
        ./gradlew benchmark:connectedAndroidTest \
          -Pandroid.testInstrumentationRunnerArguments.androidx.benchmark.suppressErrors=EMULATOR,UNLOCKED,UNSUPPORTED_ACTIVITY_MANAGER_REQUEST \
          --info

        ./gradlew macrobenchmark:connectedAndroidTest \
          -Pandroid.testInstrumentationRunnerArguments.androidx.benchmark.suppressErrors=EMULATOR,UNLOCKED,UNSUPPORTED_ACTIVITY_MANAGER_REQUEST \
          --info
      else
        echo "üìã No device available - skipping instrumented tests"
        echo "This is expected in most CI environments without emulator setup"
      fi

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    - mkdir -p reports/android
    - |
      # –ö–æ–ø–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
      if [ -d "benchmark/build/outputs/androidTest-results" ]; then
        cp -r benchmark/build/outputs/androidTest-results reports/android/micro-benchmarks/
      fi

      if [ -d "macrobenchmark/build/outputs/androidTest-results" ]; then
        cp -r macrobenchmark/build/outputs/androidTest-results reports/android/macro-benchmarks/
      fi

      # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å—Ç–∞—Ç—É—Å–∞
      echo "Android benchmarks configuration validated" > reports/android/status.txt

  artifacts:
    paths:
      - reports/android/
      - benchmark/build/outputs/
      - macrobenchmark/build/outputs/
    expire_in: 7 days

  allow_failure: true  # Android —Ç–µ—Å—Ç—ã –º–æ–≥—É—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ —ç–º—É–ª—è—Ç–æ—Ä–∞

# JMH –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
jmh-benchmarks:
  extends: .kotlin_base
  stage: test-performance
  dependencies:
    - build
  script:
    - echo "‚ö° Running JMH deep performance analysis..."

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º JMH jar
    - ./gradlew jmhJar

    # –ó–∞–ø—É—Å–∫–∞–µ–º JMH —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    - |
      java -jar build/libs/jmh.jar \
        -wi 3 \
        -i 5 \
        -f 1 \
        -r 1s \
        -w 1s \
        -rf json \
        -rff reports/jmh-results.json \
        -v EXTRA

    # –°–æ–∑–¥–∞–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á–µ—Ç
    - echo "üìä Generating JMH report..."
    - mkdir -p reports/jmh
    - mv reports/jmh-results.json reports/jmh/

    # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - |
      python3 -c "
      import json
      import sys

      try:
          with open('reports/jmh/jmh-results.json', 'r') as f:
              results = json.load(f)

          with open('reports/jmh/summary.md', 'w') as f:
              f.write('# JMH Benchmark Results\n\n')
              f.write('| Benchmark | Score | Unit | Error |\n')
              f.write('|-----------|-------|------|-------|\n')

              for result in results:
                  benchmark = result['benchmark'].split('.')[-1]
                  score = result['primaryMetric']['score']
                  unit = result['primaryMetric']['scoreUnit']
                  error = result['primaryMetric']['scoreError']
                  f.write(f'| {benchmark} | {score:.2f} | {unit} | ¬±{error:.2f} |\n')

          print('‚úÖ JMH report generated successfully')
      except Exception as e:
          print(f'‚ùå Error generating JMH report: {e}')
          sys.exit(1)
      "

  artifacts:
    paths:
      - reports/jmh/
      - build/libs/jmh.jar
    expire_in: 7 days

# –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
performance-report:
  stage: report
  image: alpine:latest
  dependencies:
    - jvm-benchmarks
    - js-benchmarks
    - native-benchmarks
    - android-benchmarks
    - jmh-benchmarks
  before_script:
    - apk add --no-cache python3 py3-pip

  script:
    - echo "üìà Generating comprehensive performance report..."

    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
    - |
      cat > final-report.md << 'EOF'
      # üöÄ Performance Test Report

      **Pipeline**: [`$CI_PIPELINE_ID`]($CI_PIPELINE_URL)
      **Commit**: [`$CI_COMMIT_SHA`]($CI_PROJECT_URL/-/commit/$CI_COMMIT_SHA)
      **Branch**: `$CI_COMMIT_REF_NAME`
      **Date**: $(date -u)

      ## üìä Test Summary

      ### JVM Benchmarks
      EOF

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º —Ç–∏–ø–µ —Ç–µ—Å—Ç–æ–≤
    - |
      for platform in jvm js native android jmh; do
        echo "### ${platform^} Results" >> final-report.md
        if [ -d "reports/$platform" ]; then
          file_count=$(find reports/$platform -type f | wc -l)
          echo "‚úÖ $file_count files generated" >> final-report.md
        else
          echo "‚ùå No results found" >> final-report.md
        fi
        echo "" >> final-report.md
      done

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    - |
      cat >> final-report.md << 'EOF'

      ## üìã Available Artifacts

      - **JVM Benchmarks**: Kotlin/JVM performance metrics
      - **JS Benchmarks**: Kotlin/JS performance metrics
      - **Native Benchmarks**: Kotlin/Native performance metrics
      - **Android Benchmarks**: Android app performance tests
      - **JMH Analysis**: Detailed microbenchmark analysis

      ## üéØ Next Steps

      1. Download artifacts from this pipeline
      2. Compare with baseline performance
      3. Investigate any performance regressions
      4. Update performance baselines if needed
      EOF

    - echo "‚úÖ Performance report generated"
    - cat final-report.md

  artifacts:
    paths:
      - final-report.md
      - reports/
    expire_in: 30 days
    reports:
      # GitLab –º–æ–∂–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
      performance: reports/*/performance.json

  # –≠—Ç–æ—Ç job –≤—Å–µ–≥–¥–∞ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞
  when: always

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö MR –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
pages:
  stage: report
  dependencies:
    - performance-report
  script:
    - mkdir public
    - cp -r reports/* public/ || echo "No reports to publish"
    - cp final-report.md public/index.md || echo "No final report"

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É
    - |
      cat > public/index.html << 'EOF'
      <!DOCTYPE html>
      <html>
      <head>
          <title>Performance Report</title>
          <meta charset="utf-8">
      </head>
      <body>
          <h1>Performance Test Results</h1>
          <p>See artifacts for detailed benchmark data.</p>
          <ul>
      EOF

    - |
      for dir in public/*/; do
        if [ -d "$dir" ]; then
          dirname=$(basename "$dir")
          echo "<li><a href=\"$dirname/\">$dirname Results</a></li>" >> public/index.html
        fi
      done

    - echo '</ul></body></html>' >> public/index.html

  artifacts:
    paths:
      - public
    expire_in: 30 days

  only:
    - main
    - develop